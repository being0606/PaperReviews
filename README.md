# Reviewed Papers Summary

This document provides an overview of the research papers reviewed, organized by detailed research fields. Each section includes a table with the paper's review date, publication venue, published year, and a link to the presentation.

---

## 1. Pretrained Transformer Models (T5, BART, BERT)

Papers discussing pretraining strategies and unified text-to-text frameworks, which have significantly influenced NLP.

| #    | Paper Title                                                                                                           | Review Date | Conference / Venue | Published Year |                                                     Link                                                     |
| :--: | :-------------------------------------------------------------------------------------------------------------------- | :---------: | :----------------: | :------------: | :----------------------------------------------------------------------------------------------------------: |
| 1    | **Seq2Seq with Attention**                                                                                            | 2024.06.29  |        ICLR        |      2015      | [Link](https://docs.google.com/presentation/d/1-iop7-Fl1rHyqmk_oOCYySII1ZMJ_5R5A8Il8RNtypw/edit#slide=id.p)  |
| 2    | **Attention Is All You Need**                                                                                         | 2025.01.17  |      NeurIPS       |      2017      | [Link](https://docs.google.com/presentation/d/1Ot4-j7qjnmUXUFDz4lnPO5yzspzc44KSsF6qlBqaQYQ/edit?usp=sharing) |
| 3    | **BERT: Bidirectional Encoder Representations from Transformers**                                                     | 2025.01.21  |       NAACL        |      2019      | [Link](https://docs.google.com/presentation/d/1jXnY-XUmqbDP-8S07ohzwKElEkweXOiZ9eEZu5x83L8/edit?usp=sharing) |
| 4    | **BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension** | 2025.01.30  |        ACL         |      2019      | [Link](https://docs.google.com/presentation/d/1G3L3qRQHZFukr5XntiWswpIZBZ15vhU4A8OAL7bvTwY/edit?usp=sharing) |
| 5    | **Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer**                                 | 2025.01.25  |        JMLR        |      2019      | [Link](https://docs.google.com/presentation/d/1s9Us2b5gyM_BHapcTuirmDaf6MmvA2Thswg2CS2Jg7o/edit?usp=sharing) |

---

## 2. MoE / Scalable Architectures

This section covers research on sparsely-gated and mixture-of-experts architectures, focusing on scalable deep learning models.

| #    | Paper Title                                                                         | Review Date | Conference / Venue | Published Year |                                                     Link                                                     |
| :--: | :---------------------------------------------------------------------------------- | :---------: | :----------------: | :------------: | :----------------------------------------------------------------------------------------------------------: |
| 1    | **Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer** | 2025.02.11  |        ICLR        |      2017      | [Link](https://docs.google.com/presentation/d/13UgUTVf9Q6mWVIRAn1f0z6UXIe7-ffRQoYmWyGYHlds/edit?usp=sharing) |
| 2    | **LoftQ: LoRA-Fine-Tuning-Aware Quantization**                                      | 2025.02.19  |        ICLR        |      2023      | [Link](https://docs.google.com/presentation/d/1tZOsxXaG-ZXeiW4eDmvEqxX6DOCKMqCcSTCFYAQPSvA/edit?usp=sharing) |

---

## 3. Reinforcement Learning and Reasoning Models

This section covers research on reinforcement learning techniques for aligning language models with human preferences, as well as studies on reasoning model architectures and their applications. It highlights methods that optimize language model behavior through human feedback and models designed for complex reasoning tasks.

| #   | Paper Title                                                                        | Review Date | Conference / Venue | Published Year |                                                     Link                                                     |
| :-- | :--------------------------------------------------------------------------------- | :---------: | :----------------: | :------------: | :----------------------------------------------------------------------------------------------------------: |
| 1   | **Direct Preference Optimization: Your Language Model is Secretly a Reward Model** | 2025.02.25  |      NeurIPS       |      2023      | [Link](https://docs.google.com/presentation/d/15VFKz5KmtCisZk_eJR2lElSIlVobWm1Ekmod4z7qpvU/edit?usp=sharing) |
| 2   | **DeepSeek R1**                                                                    | 2025.05.02  |         -          |   2025.01.22   | [Link](https://docs.google.com/presentation/d/1YBOlK6ZC5mh0R3JtpGOWMhjKEHsQTyyKRf9SSDyq6CA/edit?usp=sharing) |
| 3   | **Reasoning Model**                                                                | 2025.04.02  |         -          |       -        | [Link](https://docs.google.com/presentation/d/1PSgosh9NRIC9NvW3xN1Nu4tsguqS7gn2mQ15THMB7VM/edit?usp=sharing) |

---

## 4. Llama Models

This section provides an overview of research papers focusing on the Llama family of large language models developed by Meta AI.

| #    | Paper Title                                                                           | Review Date | Conference / Venue | Published Year |                                                     Link                                                     |
| :--: | :------------------------------------------------------------------------------------ | :---------: | :----------------- | :------------: | :----------------------------------------------------------------------------------------------------------: |
| 1    | **The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation** | 2025.04.07  | Meta               |   2025.04.05   | [Link](https://docs.google.com/presentation/d/1srpgm5Pwr8S03_gAXkLQTAApeXWttSD_MEOlUfXun78/edit?usp=sharing) |
| 2    | **LLaMA3 Code Review**                                                                | 2025.03.03  | Meta               |       -        | [Link](https://docs.google.com/presentation/d/1F4bDtjeLYN2xcN9_hJ8kNy8Iq49W2uRAZGK_CGg92qI/edit?usp=sharing) |
| 3    | **The LLaMA 3 Herd of Model**                                                         | 2025.02.24  | Meta               |   2024.07.31   | [Link](https://docs.google.com/presentation/d/1YpTnPfxIb3cOoVC9htyeNhi6Y9-aefe_FSrMrlUrfsc/edit#slide=id.p)  |

---

### Summary

This document provides a structured reference for reviewed papers, categorized by major research topics. The summaries highlight key contributions and methodologies in pretrained transformer models, scalable architectures, reinforcement learning for human feedback, and the Llama family of models.
