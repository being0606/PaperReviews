# Reviewed Papers Summary

This document provides an overview of the research papers reviewed, organized by detailed research fields. Each section includes a table with the paper's review date, publication venue, published year, and a link to the presentation.

---

## 1. Pretrained Transformer Models (T5, BART, BERT)

Papers discussing pretraining strategies and unified text-to-text frameworks, which have significantly influenced NLP.

| #    | Paper Title                                                                         | Review Date | Conference / Venue | Published Year | Link                                                                                                          |
| 1    | **Seq2Seq with Attention**                                                                                             | 2024.06.29   | ICLR (Assumed)      | 2015            | [Link](https://docs.google.com/presentation/d/1-iop7-Fl1rHyqmk_oOCYySII1ZMJ_5R5A8Il8RNtypw/edit#slide=id.p)  |
| 2    | **Attention Is All You Need**                                                                                          | 2025.01.17   | NeurIPS             | 2017            | [Link](https://docs.google.com/presentation/d/1Ot4-j7qjnmUXUFDz4lnPO5yzspzc44KSsF6qlBqaQYQ/edit?usp=sharing) |
| 3    | **BERT: Bidirectional Encoder Representations from Transformers**                                                      | 2025.01.21   | NAACL               | 2019            | [Link](https://docs.google.com/presentation/d/1jXnY-XUmqbDP-8S07ohzwKElEkweXOiZ9eEZu5x83L8/edit?usp=sharing) |
| 4    | **BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension** | 2025.01.30   | ACL    | 2019            | [Link](https://docs.google.com/presentation/d/1G3L3qRQHZFukr5XntiWswpIZBZ15vhU4A8OAL7bvTwY/edit?usp=sharing) |
| 5    | **Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer**                                  | 2025.01.25   | arXiv / JMLR        | 2019            | [Link](https://docs.google.com/presentation/d/1s9Us2b5gyM_BHapcTuirmDaf6MmvA2Thswg2CS2Jg7o/edit?usp=sharing) |

---

## 2. MoE / Scalable Architectures

This section covers research on sparsely-gated and mixture-of-experts architectures, focusing on scalable deep learning models.

| #    | Paper Title                                                                          | Review Date  | Conference / Venue  | Published Year  | Link                                                                                                          |
| ---- | ------------------------------------------------------------------------------------ | ------------ | ------------------- | --------------- | ------------------------------------------------------------------------------------------------------------- |
| 1    | **Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer**  | 2025.02.11   | ICLR                | 2017            | [Link](https://docs.google.com/presentation/d/13UgUTVf9Q6mWVIRAn1f0z6UXIe7-ffRQoYmWyGYHlds/edit?usp=sharing)  |
| 2    | **LoftQ: LoRA-Fine-Tuning-Aware Quantization**                                       | 2025.02.19   | ICLR                | 2023            | [Link](https://docs.google.com/presentation/d/1tZOsxXaG-ZXeiW4eDmvEqxX6DOCKMqCcSTCFYAQPSvA/edit?usp=sharing)  |

---

## 3. Reinforcement Learning for Human Feedback (RLHF)

This section focuses on preference optimization techniques used in fine-tuning language models to align their behavior with human preferences.

| #    | Paper Title                                                                         | Review Date  | Conference / Venue  | Published Year  | Link                                                                                                          |
| ---- | ----------------------------------------------------------------------------------- | ------------ | ------------------- | --------------- | ------------------------------------------------------------------------------------------------------------- |
| 1    | **Direct Preference Optimization: Your Language Model is Secretly a Reward Model**  | 2025.02.25   | NeurIPS             | 2023            | [Link](https://docs.google.com/presentation/d/15VFKz5KmtCisZk_eJR2lElSIlVobWm1Ekmod4z7qpvU/edit?usp=sharing)  |

---

## 4. Llama Models

This section provides an overview of research papers focusing on the Llama family of large language models developed by Meta AI.

| #    | Paper Title                                                                                                            | Review Date  | Conference / Venue  | Published Year | Link                                                                                                          |
| ---- | ---------------------------------------------------------------------------------------------------------------------- | ------------ | ------------------- | -------------- | ------------------------------------------------------------------------------------------------------------- |
| 1    | **The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation**                                  | 2025.04.07   | Meta                | 2025.04.05     | [Link](https://docs.google.com/presentation/d/1srpgm5Pwr8S03_gAXkLQTAApeXWttSD_MEOlUfXun78/edit?usp=sharing)  |
| 2    | **LLaMA3 Code Review**                                                                                                 | 2025.03.03   | Meta                | -              | [Link](https://docs.google.com/presentation/d/1F4bDtjeLYN2xcN9_hJ8kNy8Iq49W2uRAZGK_CGg92qI/edit?usp=sharing)  |
| 3    | **The LLaMA 3 Herd of Model**                                                                                          | 2025.02.24   | Meta                | 2024.07.31     | [Link](https://docs.google.com/presentation/d/1YpTnPfxIb3cOoVC9htyeNhi6Y9-aefe_FSrMrlUrfsc/edit#slide=id.p)   |

---

### Summary

This document provides a structured reference for reviewed papers, categorized by major research topics. The summaries highlight key contributions and methodologies in pretrained transformer models, scalable architectures, reinforcement learning for human feedback, and the Llama family of models.
